{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dand_p5_fraudDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.13 |Anaconda custom (x86_64)| (default, Dec 20 2016, 23:05:08) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The original code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the 1st part of that code\n",
    "\n",
    "* Read file into pandas and then use dataframe to do all kinds of things like in kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tree/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary'] # You will need to use more features\n",
    "features_list = [] # create a new list to incorporate the columns\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "#     print len()\n",
    "#     for key, value in data_dict.items():\n",
    "#         print key , ':\\t' , value\n",
    "#         print '\\n\\n'\n",
    "    dataset_dict = pd.read_pickle('final_project_dataset.pkl')\n",
    "        \n",
    "# read them into pandas dataframe\n",
    "dataset_df_temp = pd.DataFrame(dataset_dict)\n",
    "dataset_df = pd.DataFrame.transpose(dataset_df_temp)\n",
    "dataset_df.shape\n",
    "dataset_df\n",
    "\n",
    "# get the features and have a look\n",
    "print features_list\n",
    "print len(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clean dataset\n",
    "* most of the are numerical data, so can use some ways to do this \n",
    "* email and poi are not, do it later\n",
    "* use KNN from fancyimputation package to fillin the NaN (fancyimpute is not included generally, so fill with 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    128\n",
       "True      18\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df['poi'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Drop outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop shape \t\t:  (146, 21)\n",
      "Before drop the row is \t\t:  Index([u'TOTAL'], dtype='object')\n",
      "After drop shape \t\t :  (145, 21)\n",
      "After drop the row is \t\t:  Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print 'Before drop shape \\t\\t: ' , dataset_df.shape\n",
    "print 'Before drop the row is \\t\\t: ' , dataset_df.query(''' salary > 2.5e7 and salary != 'NaN' ''').index\n",
    "dataset_df.drop(dataset_df.query(''' salary > 2.5e7 and salary != 'NaN' ''').index, inplace=True)\n",
    "print 'After drop shape \\t\\t : ' , dataset_df.shape\n",
    "print 'After drop the row is \\t\\t: ' , dataset_df.query(''' salary > 2.5e7 and salary != 'NaN' ''').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonus                        float64\n",
      "deferral_payments            float64\n",
      "deferred_income              float64\n",
      "director_fees                float64\n",
      "email_address                 object\n",
      "exercised_stock_options      float64\n",
      "expenses                     float64\n",
      "from_messages                float64\n",
      "from_poi_to_this_person      float64\n",
      "from_this_person_to_poi      float64\n",
      "loan_advances                float64\n",
      "long_term_incentive          float64\n",
      "other                        float64\n",
      "poi                             bool\n",
      "restricted_stock             float64\n",
      "restricted_stock_deferred    float64\n",
      "salary                       float64\n",
      "shared_receipt_with_poi      float64\n",
      "to_messages                  float64\n",
      "total_payments               float64\n",
      "total_stock_value            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print dataset_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can see this is also an outlier\n",
    "dataset_df.query(''' total_payments > 1e8 ''').shape\n",
    "dataset_df.drop(dataset_df.query(''' total_payments > 1e8 ''').index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 21)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. try do copy the True values \n",
    "> The labels are very skewed and the algorithm cannot get good combination of precision and recall\n",
    "in order to get enought precison and recall, I append some of the true values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Turns out this worked. \n",
    "Used 15 times, gets me 0.412 and 0.404 as precision and recall respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 21)\n"
     ]
    }
   ],
   "source": [
    "poi_trues = dataset_df.query(''' poi == True ''')\n",
    "print poi_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 21)\n"
     ]
    }
   ],
   "source": [
    "dataset_df_temp = dataset_df\n",
    "for ii in range(16):\n",
    "    dataset_df_temp = dataset_df_temp.append(poi_trues)\n",
    "dataset_df_dup = dataset_df_temp\n",
    "print dataset_df_dup.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ====================  Now the dataset is much better     ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. the NaN in this table are simply the string saying 'NaN'\n",
    "* <font color='red'>Remember don't change the NaN in </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try to use the dataset with replica of True values\n",
    "dataset_df_removeNaN_zero = dataset_df_dup.replace('NaN', 0, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fill 'NaN' with 0\n",
    "# dataset_df_removeNaN_zero = dataset_df.replace('NaN', 0, inplace=False)\n",
    "\n",
    "# fill 'NaN' with np.nan\n",
    "# dataset_df_removeNaN_npnan = dataset_df.replace('NaN', np.nan, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>email_address</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>...</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>poi</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALLEN PHILLIP K</th>\n",
       "      <td>4175000.0</td>\n",
       "      <td>2869717.0</td>\n",
       "      <td>-3081055.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>1729541.0</td>\n",
       "      <td>13868.0</td>\n",
       "      <td>2195.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>304805.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>False</td>\n",
       "      <td>126027.0</td>\n",
       "      <td>-126027.0</td>\n",
       "      <td>201955.0</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>2902.0</td>\n",
       "      <td>4484442.0</td>\n",
       "      <td>1729541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>0.0</td>\n",
       "      <td>178980.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>257817.0</td>\n",
       "      <td>3486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182466.0</td>\n",
       "      <td>257817.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bonus  deferral_payments  deferred_income  director_fees  \\\n",
       "ALLEN PHILLIP K  4175000.0          2869717.0       -3081055.0            0.0   \n",
       "BADUM JAMES P          0.0           178980.0              0.0            0.0   \n",
       "\n",
       "                           email_address  exercised_stock_options  expenses  \\\n",
       "ALLEN PHILLIP K  phillip.allen@enron.com                1729541.0   13868.0   \n",
       "BADUM JAMES P                          0                 257817.0    3486.0   \n",
       "\n",
       "                 from_messages  from_poi_to_this_person  \\\n",
       "ALLEN PHILLIP K         2195.0                     47.0   \n",
       "BADUM JAMES P              0.0                      0.0   \n",
       "\n",
       "                 from_this_person_to_poi        ...          \\\n",
       "ALLEN PHILLIP K                     65.0        ...           \n",
       "BADUM JAMES P                        0.0        ...           \n",
       "\n",
       "                 long_term_incentive  other    poi restricted_stock  \\\n",
       "ALLEN PHILLIP K             304805.0  152.0  False         126027.0   \n",
       "BADUM JAMES P                    0.0    0.0  False              0.0   \n",
       "\n",
       "                 restricted_stock_deferred    salary  shared_receipt_with_poi  \\\n",
       "ALLEN PHILLIP K                  -126027.0  201955.0                   1407.0   \n",
       "BADUM JAMES P                          0.0       0.0                      0.0   \n",
       "\n",
       "                 to_messages  total_payments  total_stock_value  \n",
       "ALLEN PHILLIP K       2902.0       4484442.0          1729541.0  \n",
       "BADUM JAMES P            0.0        182466.0           257817.0  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look at the table with 0 replacing the NaN\n",
    "dataset_df_removeNaN_zero.head(2)\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# not good at all\n",
    "# because there are too many 'NaN', so if use 0 to fill\n",
    "# there will be too much 0 and this would have a bad influence on the performance\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# have a look at the table with np.nan replacing the NaN\n",
    "# dataset_df_removeNaN_npnan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_var\t\t['bonus', 'deferral_payments', 'deferred_income', 'director_fees', 'exercised_stock_options', 'expenses', 'from_messages', 'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'restricted_stock_deferred', 'salary', 'shared_receipt_with_poi', 'to_messages', 'total_payments', 'total_stock_value']\n",
      "non_num_var\t['email_address', 'poi']\n"
     ]
    }
   ],
   "source": [
    "# collect the numerical columns to get ready for imputation\n",
    "# leave the string type columns alone \n",
    "\n",
    "num_var = []\n",
    "non_num_var = []\n",
    "for each_column in dataset_df_removeNaN_zero:    \n",
    "    if each_column != 'email_address' and each_column != 'poi':\n",
    "        num_var.append(each_column)\n",
    "    else :\n",
    "        non_num_var.append(each_column)\n",
    "\n",
    "print 'num_var\\t\\t' , num_var\n",
    "print 'non_num_var\\t' , non_num_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     289\n",
       "False    127\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df_removeNaN_zero['poi'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the numerical values for scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_df_removeNaN_zero_num = dataset_df_removeNaN_zero[num_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Scaling the numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "dataset_filled_knn_df_scaled_vals = ss.fit_transform(dataset_df_removeNaN_zero_num.values)\n",
    "dataset_filled_knn_df_scaled = pd.DataFrame(dataset_filled_knn_df_scaled_vals, \\\n",
    "                                            index=dataset_df_removeNaN_zero_num.index,\\\n",
    "                                            columns=dataset_df_removeNaN_zero_num.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALLEN PHILLIP K</th>\n",
       "      <td>1.968379</td>\n",
       "      <td>4.485466</td>\n",
       "      <td>-2.548274</td>\n",
       "      <td>-0.182251</td>\n",
       "      <td>-0.313287</td>\n",
       "      <td>-0.874188</td>\n",
       "      <td>1.853617</td>\n",
       "      <td>-0.179612</td>\n",
       "      <td>0.170923</td>\n",
       "      <td>-0.057789</td>\n",
       "      <td>-0.318114</td>\n",
       "      <td>-0.431808</td>\n",
       "      <td>-0.642469</td>\n",
       "      <td>-0.198181</td>\n",
       "      <td>-0.313615</td>\n",
       "      <td>0.237190</td>\n",
       "      <td>0.660394</td>\n",
       "      <td>1.079383</td>\n",
       "      <td>-0.425047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>-0.817250</td>\n",
       "      <td>0.017295</td>\n",
       "      <td>0.485190</td>\n",
       "      <td>-0.182251</td>\n",
       "      <td>-0.504224</td>\n",
       "      <td>-1.128066</td>\n",
       "      <td>-0.277831</td>\n",
       "      <td>-0.780736</td>\n",
       "      <td>-0.352967</td>\n",
       "      <td>-0.057789</td>\n",
       "      <td>-0.757238</td>\n",
       "      <td>-0.432087</td>\n",
       "      <td>-0.716930</td>\n",
       "      <td>-0.033258</td>\n",
       "      <td>-1.229791</td>\n",
       "      <td>-0.866993</td>\n",
       "      <td>-0.763059</td>\n",
       "      <td>-0.811414</td>\n",
       "      <td>-0.598031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    bonus  deferral_payments  deferred_income  director_fees  \\\n",
       "ALLEN PHILLIP K  1.968379           4.485466        -2.548274      -0.182251   \n",
       "BADUM JAMES P   -0.817250           0.017295         0.485190      -0.182251   \n",
       "\n",
       "                 exercised_stock_options  expenses  from_messages  \\\n",
       "ALLEN PHILLIP K                -0.313287 -0.874188       1.853617   \n",
       "BADUM JAMES P                  -0.504224 -1.128066      -0.277831   \n",
       "\n",
       "                 from_poi_to_this_person  from_this_person_to_poi  \\\n",
       "ALLEN PHILLIP K                -0.179612                 0.170923   \n",
       "BADUM JAMES P                  -0.780736                -0.352967   \n",
       "\n",
       "                 loan_advances  long_term_incentive     other  \\\n",
       "ALLEN PHILLIP K      -0.057789            -0.318114 -0.431808   \n",
       "BADUM JAMES P        -0.057789            -0.757238 -0.432087   \n",
       "\n",
       "                 restricted_stock  restricted_stock_deferred    salary  \\\n",
       "ALLEN PHILLIP K         -0.642469                  -0.198181 -0.313615   \n",
       "BADUM JAMES P           -0.716930                  -0.033258 -1.229791   \n",
       "\n",
       "                 shared_receipt_with_poi  to_messages  total_payments  \\\n",
       "ALLEN PHILLIP K                 0.237190     0.660394        1.079383   \n",
       "BADUM JAMES P                  -0.866993    -0.763059       -0.811414   \n",
       "\n",
       "                 total_stock_value  \n",
       "ALLEN PHILLIP K          -0.425047  \n",
       "BADUM JAMES P            -0.598031  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filled_knn_df_scaled.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-way interaction numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns before interaction 19\n",
      "number of columns after interaction 703\n"
     ]
    }
   ],
   "source": [
    "print 'number of columns before interaction' , len(dataset_filled_knn_df_scaled.columns)\n",
    "\n",
    "original_num_of_cols = len(dataset_filled_knn_df_scaled.columns)\n",
    "\n",
    "# 2 way plus, minus, times\n",
    "for outer_iter in range(original_num_of_cols):\n",
    "    for inner_iter in range(outer_iter + 1, original_num_of_cols):\n",
    "        first_name = dataset_filled_knn_df_scaled.columns[outer_iter]\n",
    "        last_name = dataset_filled_knn_df_scaled.columns[inner_iter]\n",
    "        dataset_filled_knn_df_scaled[first_name + '_' + last_name + '_' + 'plus'] \\\n",
    "                        = dataset_filled_knn_df_scaled[first_name] + dataset_filled_knn_df_scaled[last_name]\n",
    "            \n",
    "        dataset_filled_knn_df_scaled[first_name + '_' + last_name + '_' + 'minus'] \\\n",
    "                        = dataset_filled_knn_df_scaled[first_name] - dataset_filled_knn_df_scaled[last_name]\n",
    "            \n",
    "        dataset_filled_knn_df_scaled[first_name + '_' + last_name + '_' + 'times'] \\\n",
    "                        = dataset_filled_knn_df_scaled[first_name] * dataset_filled_knn_df_scaled[last_name]\n",
    "        \n",
    "        dataset_filled_knn_df_scaled[first_name + '_' + last_name + '_' + 'divide'] \\\n",
    "                        = dataset_filled_knn_df_scaled[first_name] / dataset_filled_knn_df_scaled[last_name]\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "dataset_filled_knn_df_scaled.shape\n",
    "\n",
    "print 'number of columns after interaction' , len(dataset_filled_knn_df_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 703)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filled_knn_df_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "dataset_filled_knn_df_scaled_vals_again_vals = ss.fit_transform(dataset_filled_knn_df_scaled.values)\n",
    "dataset_filled_knn_df_scaled_vals_again = pd.DataFrame(dataset_filled_knn_df_scaled_vals_again_vals, \\\n",
    "                                            index=dataset_filled_knn_df_scaled.index,\\\n",
    "                                            columns=dataset_filled_knn_df_scaled.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StratifiedShuffleSplit due to the skewedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = dataset_filled_knn_df_scaled_vals_again\n",
    "Y_all = dataset_df_dup['poi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tried so many times, found dtc is the most potential one, so put work on decision tree classifier\n",
    "#### Could further tune it, but use as default can get the result as required, so I guess it okay for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Choose a model from all the models\n",
    "* Importing\n",
    "* Creating\n",
    "* reshape to np array for the StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "knc = KNeighborsClassifier()\n",
    "svc = SVC()\n",
    "gpc = GaussianProcessClassifier()\n",
    "dtc = DecisionTreeClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "abc = AdaBoostClassifier()\n",
    "nbc = GaussianNB()\n",
    "\n",
    "X_array = np.array(X_all.values)\n",
    "Y_array = np.array(Y_all.reshape(-1,1))\n",
    "sss = StratifiedShuffleSplit(n_splits=100, test_size=0.1, random_state=2017)\n",
    "for train_index, test_index in sss.split(X_array, Y_array):\n",
    "    x_train, x_test = X_array[train_index], X_array[test_index]\n",
    "    y_train, y_test = Y_array[train_index], Y_array[test_index]\n",
    "\n",
    "    mlp.fit(x_train, y_train)\n",
    "    knc.fit(x_train, y_train)\n",
    "    svc.fit(x_train, y_train)\n",
    "    gpc.fit(x_train, y_train)\n",
    "    dtc.fit(x_train, y_train)\n",
    "    rfc.fit(x_train, y_train)\n",
    "    abc.fit(x_train, y_train)\n",
    "    nbc.fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "#     from sklearn.metrics import classification_report\n",
    "#     clf = dtc\n",
    "#     y_pred = clf.predict(x_test).ravel()\n",
    "#     y_true = y_test\n",
    "#     print classification_report(y_true, y_pred)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.00', '1.00', '1.00']\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_true, y_pred)).split()[-4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "pre_rec_f1 = classification_report(y_true, y_pred).split()[-4:-1]\n",
    "int_pre_rec_f1 = [float(ii) for ii in pre_rec_f1]\n",
    "print int_pre_rec_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2. Grid Search after choose decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "       fit_params={}, iid=False, n_jobs=-1,\n",
       "       param_grid={'min_samples_split': [2, 4, 6, 8], 'random_state': [2017], 'max_depth': [5, 8, 10, 15, 20], 'min_samples_leaf': [1, 2, 3, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'max_depth': [5, 8, 10, 15, 20],\n",
    "          'min_samples_split': [2, 4, 6, 8],\n",
    "          'min_samples_leaf': [1, 2, 3, 4],\n",
    "          'random_state': [2017]}\n",
    "gsearch = GridSearchCV(estimator = dtc, \n",
    "                       param_grid = params, scoring='roc_auc', n_jobs=-1, iid=False, cv=5)\n",
    "\n",
    "gsearch.fit(X_all, Y_all.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 2, 'random_state': 2017, 'max_depth': 10, 'min_samples_leaf': 2}\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=2017, splitter='best')\n",
      "0.954904183536\n"
     ]
    }
   ],
   "source": [
    "print gsearch.best_params_\n",
    "print gsearch.best_estimator_\n",
    "print gsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00        13\n",
      "       True       1.00      1.00      1.00        29\n",
      "\n",
      "avg / total       1.00      1.00      1.00        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = gsearch.predict(x_test).ravel()\n",
    "y_true = y_test\n",
    "print classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following are the format thing\n",
    "* Convert all of the results to the form required and output for the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the output format, I have to append the poi to the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_filled_knn_df_scaled.insert(loc=0, column='poi', value=dataset_df_dup['poi'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_list = list(dataset_filled_knn_df_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dataset = dataset_filled_knn_df_scaled.transpose().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          3.194632    3.52404595 ...,  1.3790878  -0.21008601\n",
      "  -6.90812155]\n",
      " [ 0.         -0.56496234 -0.05547462 ..., -0.07514396  0.26669429\n",
      "   1.15647901]\n",
      " [ 0.         -0.56496234 -0.29357392 ..., -0.81099109 -0.14182925\n",
      "  -0.45908994]\n",
      " ..., \n",
      " [ 0.         -0.56496234 -0.29357392 ..., -0.12513246  0.31808184\n",
      "   1.24784545]\n",
      " [ 1.         -0.56496234 -0.29357392 ..., -2.41850053 -0.93427216\n",
      "  -0.24929258]\n",
      " [ 0.         -0.56496234 -0.29357392 ..., -0.11373478  0.29993124\n",
      "   1.230355  ]]\n"
     ]
    }
   ],
   "source": [
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3)\n",
      "(10, 1)\n",
      "(8, 3) (2, 3)\n",
      "(8, 3) (2, 3)\n",
      "(8, 3) (2, 3)\n",
      "(8, 3) (2, 3)\n",
      "(8, 3) (2, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "x = np.array([[1,2,3], [4,5,6], [7,8,9], [2,4,6], [7,4,2], [2,3,4], [1,1,1], [2,2,2], [4,6,3], [9,4,2]])\n",
    "y = np.array([[1], [0], [1], [1], [0], [1], [0], [1], [0], [1]])\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=2017)\n",
    "\n",
    "print x.shape\n",
    "print y.shape\n",
    "for train_index, test_index in sss.split(x, y):\n",
    "    x_tr, x_te = x[train_index], x[test_index]\n",
    "    y_tr, y_te = y[train_index], y[test_index]\n",
    "    \n",
    "    print x_tr.shape, x_te.shape\n",
    "\n",
    "# It shuffles but not stratified, means it could be a subset has no True lable at all\n",
    "# That's not good for the model to fit because the label set is skewed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
